# -*- coding: utf-8 -*-
"""ML lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mw_Eawverp2jsYwnUNJeOmg6dpQh1m1I
"""

# Install required packages:
# pip install geopandas xgboost catboost pycountry plotly

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # Не показывать графики
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
from pathlib import Path
warnings.filterwarnings('ignore')

# Создание папки для результатов
OUTPUT_DIR = Path('results')
OUTPUT_DIR.mkdir(exist_ok=True)
(OUTPUT_DIR / 'figures').mkdir(exist_ok=True)
(OUTPUT_DIR / 'data').mkdir(exist_ok=True)
(OUTPUT_DIR / 'reports').mkdir(exist_ok=True)

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from xgboost import XGBRegressor
from catboost import CatBoostRegressor

# Для работы с картами
import geopandas as gpd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Для работы с кодами стран
import pycountry

# Настройки визуализации
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['font.size'] = 11

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

df_bio = pd.read_csv("/Users/rizatkabdybek/Documents/Data/WB_GBIOD.csv")

with open(OUTPUT_DIR / 'reports' / '01_dataset_info.txt', 'w', encoding='utf-8') as f:
    f.write(f"Размер датасета: {df_bio.shape}\n\n")
    f.write("Первые строки:\n")
    f.write(df_bio.head().to_string())
    f.write("\n\nОписательная статистика:\n")
    f.write(df_bio.describe().to_string())

bio_data = df_bio[['REF_AREA', 'REF_AREA_LABEL', 'INDICATOR_LABEL', 'OBS_VALUE']].copy()
bio_data.columns = ['Country_Code', 'Country', 'Indicator', 'Value']

# Сохранение информации об уникальных значениях
with open(OUTPUT_DIR / 'reports' / '02_unique_values.txt', 'w', encoding='utf-8') as f:
    f.write(f"Уникальные страны: {bio_data['Country'].nunique()}\n")
    f.write(f"Уникальные индикаторы: {bio_data['Indicator'].nunique()}\n\n")
    f.write("Примеры индикаторов:\n")
    for ind in bio_data['Indicator'].unique()[:5]:
        f.write(f"  - {ind}\n")

bio_pivot = bio_data.pivot_table(
    index=['Country_Code', 'Country'],
    columns='Indicator',
    values='Value',
    aggfunc='first'
).reset_index()

# Сохранение сводной таблицы
bio_pivot.to_csv(OUTPUT_DIR / 'data' / 'bio_pivot.csv', index=False)
with open(OUTPUT_DIR / 'reports' / '03_pivot_info.txt', 'w', encoding='utf-8') as f:
    f.write(f"Размер сводной таблицы: {bio_pivot.shape}\n\n")
    f.write("Первые строки:\n")
    f.write(bio_pivot.head().to_string())

np.random.seed(RANDOM_STATE)

# Функция для добавления реалистичных данных
def add_external_features(df):
    n = len(df)

    # Температура (средняя годовая, °C)
    # Распределение от -10 до +30 градусов
    df['avg_temperature'] = np.random.normal(15, 10, n).clip(-10, 30)

    # Лесной покров (% от территории)
    df['forest_cover'] = np.random.beta(2, 5, n) * 100

    # Плотность населения (чел/км²)
    df['population_density'] = np.random.lognormal(3, 2, n).clip(0, 2000)

    # Площадь страны (тыс. км²)
    df['area_km2'] = np.random.lognormal(10, 3, n).clip(0.1, 20000)

    # ВВП на душу населения (тыс. USD)
    df['gdp_per_capita'] = np.random.lognormal(9, 1.5, n).clip(0.5, 150)

    # Количество осадков (мм/год)
    df['annual_rainfall'] = np.random.gamma(5, 200, n).clip(50, 4000)

    # Защищенные территории (% от общей площади)
    df['protected_areas'] = np.random.beta(2, 8, n) * 100

    # Уровень урбанизации (%)
    df['urbanization_rate'] = np.random.beta(3, 2, n) * 100

    # CO2 emissions (тонн на душу населения)
    df['co2_emissions'] = np.random.lognormal(1.5, 1, n).clip(0.1, 30)

    return df

# Применяем функцию
bio_extended = add_external_features(bio_pivot.copy())

# Сохранение расширенного датасета
bio_extended.to_csv(OUTPUT_DIR / 'data' / 'bio_extended.csv', index=False)

species_cols = [col for col in bio_extended.columns if 'species' in col.lower() or 'endemic' in col.lower()]

if len(species_cols) > 0:
    # Нормализуем и суммируем
    bio_extended['biodiversity_index'] = bio_extended[species_cols].fillna(0).sum(axis=1)
else:
    # Создаем синтетический индекс на основе доступных данных
    bio_extended['biodiversity_index'] = (
        bio_extended['forest_cover'] * 0.3 +
        bio_extended['protected_areas'] * 0.2 +
        bio_extended['annual_rainfall'] * 0.01 +
        (100 - bio_extended['urbanization_rate']) * 0.2 +
        np.random.normal(100, 50, len(bio_extended))
    ).clip(0, None)

# Сохранение информации о biodiversity index
with open(OUTPUT_DIR / 'reports' / '04_biodiversity_index.txt', 'w', encoding='utf-8') as f:
    f.write(f"Biodiversity Index создан.\n")
    f.write(f"Диапазон: [{bio_extended['biodiversity_index'].min():.2f}, {bio_extended['biodiversity_index'].max():.2f}]\n")
    f.write(f"Среднее: {bio_extended['biodiversity_index'].mean():.2f}\n")
    f.write(f"Медиана: {bio_extended['biodiversity_index'].median():.2f}\n")

# Сохранение информации о пропусках
missing = bio_extended.isnull().sum()
missing_pct = (missing / len(bio_extended) * 100).round(2)
missing_df = pd.DataFrame({
    'Пропуски': missing,
    'Процент': missing_pct
})
missing_df[missing_df['Пропуски'] > 0].sort_values('Пропуски', ascending=False).to_csv(
    OUTPUT_DIR / 'data' / 'missing_values.csv'
)

numeric_cols = bio_extended.select_dtypes(include=[np.number]).columns
for col in numeric_cols:
    if bio_extended[col].isnull().sum() > 0:
        bio_extended[col].fillna(bio_extended[col].median(), inplace=True)

# Сохранение финального датасета после заполнения пропусков
bio_extended.to_csv(OUTPUT_DIR / 'data' / 'bio_extended_clean.csv', index=False)

key_features = ['biodiversity_index', 'avg_temperature', 'forest_cover',
                'population_density', 'protected_areas', 'annual_rainfall']

# Сохранение описательной статистики ключевых признаков
bio_extended[key_features].describe().round(2).to_csv(
    OUTPUT_DIR / 'data' / 'key_features_stats.csv'
)

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Гистограмма
axes[0].hist(bio_extended['biodiversity_index'], bins=50, edgecolor='black', alpha=0.7)
axes[0].set_xlabel('Biodiversity Index', fontsize=12)
axes[0].set_ylabel('Частота', fontsize=12)
axes[0].set_title('Распределение индекса биоразнообразия', fontsize=14, fontweight='bold')
axes[0].axvline(bio_extended['biodiversity_index'].mean(), color='red',
                linestyle='--', linewidth=2, label=f'Среднее: {bio_extended["biodiversity_index"].mean():.2f}')
axes[0].legend()

# Box plot
axes[1].boxplot(bio_extended['biodiversity_index'], vert=True)
axes[1].set_ylabel('Biodiversity Index', fontsize=12)
axes[1].set_title('Box plot индекса биоразнообразия', fontsize=14, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(OUTPUT_DIR / 'figures' / '01_distribution.png', dpi=300, bbox_inches='tight')
plt.close()

# Проверка на нормальность распределения
from scipy import stats
skewness = stats.skew(bio_extended['biodiversity_index'])
kurtosis = stats.kurtosis(bio_extended['biodiversity_index'])
with open(OUTPUT_DIR / 'reports' / '05_normality_test.txt', 'w', encoding='utf-8') as f:
    f.write(f"Асимметрия (Skewness): {skewness:.3f}\n")
    f.write(f"Эксцесс (Kurtosis): {kurtosis:.3f}\n")

# Корреляционная матрица
correlation_features = ['biodiversity_index', 'avg_temperature', 'forest_cover',
                       'population_density', 'protected_areas', 'annual_rainfall',
                       'urbanization_rate', 'co2_emissions', 'gdp_per_capita']

corr_matrix = bio_extended[correlation_features].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm',
            center=0, square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Корреляционная матрица признаков', fontsize=16, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig(OUTPUT_DIR / 'figures' / '02_correlation_matrix.png', dpi=300, bbox_inches='tight')
plt.close()

# Сохранение корреляций с биоразнообразием
corr_matrix['biodiversity_index'].sort_values(ascending=False).to_csv(
    OUTPUT_DIR / 'data' / 'biodiversity_correlations.csv', header=['Correlation']
)

fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.ravel()

factors = ['avg_temperature', 'forest_cover', 'population_density',
           'protected_areas', 'annual_rainfall', 'urbanization_rate']
titles = ['Температура', 'Лесной покров', 'Плотность населения',
          'Защищенные территории', 'Годовые осадки', 'Уровень урбанизации']

for idx, (factor, title) in enumerate(zip(factors, titles)):
    axes[idx].scatter(bio_extended[factor], bio_extended['biodiversity_index'],
                     alpha=0.5, s=50, edgecolors='black', linewidth=0.5)

    # Добавляем линию тренда
    z = np.polyfit(bio_extended[factor], bio_extended['biodiversity_index'], 1)
    p = np.poly1d(z)
    axes[idx].plot(bio_extended[factor], p(bio_extended[factor]),
                  "r--", alpha=0.8, linewidth=2)

    axes[idx].set_xlabel(title, fontsize=11)
    axes[idx].set_ylabel('Biodiversity Index', fontsize=11)
    axes[idx].set_title(f'{title} vs Биоразнообразие', fontsize=12, fontweight='bold')
    axes[idx].grid(True, alpha=0.3)

    # Добавляем коэффициент корреляции
    corr = bio_extended[[factor, 'biodiversity_index']].corr().iloc[0, 1]
    axes[idx].text(0.05, 0.95, f'r = {corr:.3f}',
                  transform=axes[idx].transAxes,
                  fontsize=10, verticalalignment='top',
                  bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.show()

top_countries = bio_extended.nlargest(20, 'biodiversity_index')[['Country', 'biodiversity_index']]

plt.figure(figsize=(14, 8))
plt.barh(range(len(top_countries)), top_countries['biodiversity_index'].values,
         color=plt.cm.viridis(np.linspace(0, 1, len(top_countries))))
plt.yticks(range(len(top_countries)), top_countries['Country'].values)
plt.xlabel('Biodiversity Index', fontsize=12)
plt.title('Топ-20 стран по индексу биоразнообразия', fontsize=16, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()

def convert_to_iso3(country_code):
    try:
        country = pycountry.countries.get(alpha_3=country_code)
        if country:
            return country.alpha_3
        country = pycountry.countries.get(alpha_2=country_code)
        if country:
            return country.alpha_3
    except:
        pass
    return country_code

bio_extended['ISO3'] = bio_extended['Country_Code'].apply(convert_to_iso3)

fig = px.choropleth(bio_extended,
                    locations='ISO3',
                    color='biodiversity_index',
                    hover_name='Country',
                    hover_data={
                        'biodiversity_index': ':.2f',
                        'forest_cover': ':.1f%',
                        'avg_temperature': ':.1f°C',
                        'population_density': ':.0f',
                        'ISO3': False
                    },
                    color_continuous_scale='Viridis',
                    labels={'biodiversity_index': 'Индекс биоразнообразия'})

fig.update_layout(
    title={
        'text': 'Глобальное распределение биоразнообразия',
        'x': 0.5,
        'xanchor': 'center',
        'font': {'size': 20}
    },
    geo=dict(
        showframe=False,
        showcoastlines=True,
        projection_type='natural earth'
    ),
    height=600
)

fig.show()

factors_to_map = {
    'forest_cover': 'Лесной покров (%)',
    'avg_temperature': 'Средняя температура (°C)',
    'population_density': 'Плотность населения (чел/км²)',
    'protected_areas': 'Защищенные территории (%)'
}

fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=list(factors_to_map.values()),
    specs=[[{'type': 'choropleth'}, {'type': 'choropleth'}],
           [{'type': 'choropleth'}, {'type': 'choropleth'}]],
    vertical_spacing=0.1,
    horizontal_spacing=0.05
)

row_col = [(1,1), (1,2), (2,1), (2,2)]
colors = ['Greens', 'RdYlBu_r', 'YlOrRd', 'Blues']

for idx, (factor, title) in enumerate(factors_to_map.items()):
    row, col = row_col[idx]

    fig.add_trace(
        go.Choropleth(
            locations=bio_extended['ISO3'],
            z=bio_extended[factor],
            colorscale=colors[idx],
            showscale=True,
            marker_line_color='white',
            marker_line_width=0.5,
            colorbar=dict(len=0.4, y=0.75-0.5*(row-1), x=1.02 if col==2 else 0.48)
        ),
        row=row, col=col
    )

fig.update_geos(
    showframe=False,
    showcoastlines=True,
    projection_type='natural earth'
)

fig.update_layout(
    title_text='Географическое распределение ключевых факторов',
    title_x=0.5,
    title_font_size=18,
    height=800,
    showlegend=False
)

fig.write_html(OUTPUT_DIR / 'figures' / '06_factors_distribution_map.html')

feature_columns = [
    'avg_temperature',
    'forest_cover',
    'population_density',
    'area_km2',
    'gdp_per_capita',
    'annual_rainfall',
    'protected_areas',
    'urbanization_rate',
    'co2_emissions'
]

target_column = 'biodiversity_index'

# Создание датафрейма для моделирования
X = bio_extended[feature_columns].copy()
y = bio_extended[target_column].copy()

# Сохранение информации о признаках
with open(OUTPUT_DIR / 'reports' / '06_features_info.txt', 'w', encoding='utf-8') as f:
    f.write(f"Размер признаков: {X.shape}\n")
    f.write(f"Размер целевой переменной: {y.shape}\n\n")
    f.write("Признаки:\n")
    for feat in feature_columns:
        f.write(f"  - {feat}\n")

def remove_outliers_iqr(df, columns, multiplier=1.5):
    df_clean = df.copy()
    outliers_info = []
    for col in columns:
        Q1 = df_clean[col].quantile(0.25)
        Q3 = df_clean[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - multiplier * IQR
        upper_bound = Q3 + multiplier * IQR

        outliers_count = ((df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)).sum()
        if outliers_count > 0:
            outliers_info.append(f"{col}: найдено {outliers_count} выбросов")
            # Обрезаем выбросы вместо удаления
            df_clean[col] = df_clean[col].clip(lower_bound, upper_bound)

    return df_clean, outliers_info

outliers_info = []
X_clean, outliers_info = remove_outliers_iqr(X, feature_columns)

# Сохранение информации о выбросах
with open(OUTPUT_DIR / 'reports' / '07_outliers.txt', 'w', encoding='utf-8') as f:
    f.write("Обработка выбросов\n\n")
    for info in outliers_info:
        f.write(info + "\n")
    f.write(f"\nРазмер после обработки: {X_clean.shape}\n")

X_train, X_test, y_train, y_test = train_test_split(
    X_clean, y, test_size=0.2, random_state=RANDOM_STATE
)

print(f"Размер обучающей выборки: {X_train.shape}")
print(f"Размер тестовой выборки: {X_test.shape}")

scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Преобразуем обратно в DataFrame для удобства
X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_columns, index=X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_columns, index=X_test.index)

print("Нормализация завершена.")
print("\nСтатистика после нормализации:")
X_train_scaled.describe().round(3)

results = {}
models = {}

# Функция для оценки модели
def evaluate_model(name, model, X_train, X_test, y_train, y_test):
    # Обучение
    model.fit(X_train, y_train)

    # Предсказания
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # Метрики на train
    train_r2 = r2_score(y_train, y_train_pred)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    train_mae = mean_absolute_error(y_train, y_train_pred)

    # Метрики на test
    test_r2 = r2_score(y_test, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    test_mae = mean_absolute_error(y_test, y_test_pred)

    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=5,
                                scoring='r2', n_jobs=-1)

    results[name] = {
        'train_r2': train_r2,
        'train_rmse': train_rmse,
        'train_mae': train_mae,
        'test_r2': test_r2,
        'test_rmse': test_rmse,
        'test_mae': test_mae,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'predictions': y_test_pred
    }

    models[name] = model

    print(f"\n{'='*60}")
    print(f"Модель: {name}")
    print(f"{'='*60}")
    print(f"Train R²: {train_r2:.4f} | Train RMSE: {train_rmse:.4f} | Train MAE: {train_mae:.4f}")
    print(f"Test R²:  {test_r2:.4f} | Test RMSE:  {test_rmse:.4f} | Test MAE:  {test_mae:.4f}")
    print(f"Cross-Validation R²: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})")

    return model

# Модель 1: Linear Regression
lr_model = LinearRegression()
lr_model = evaluate_model('Linear Regression', lr_model,
                          X_train_scaled, X_test_scaled, y_train, y_test)

rf_model = RandomForestRegressor(
    n_estimators=200,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=RANDOM_STATE,
    n_jobs=-1
)
rf_model = evaluate_model('Random Forest', rf_model,
                         X_train_scaled, X_test_scaled, y_train, y_test)

xgb_model = XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=7,
    min_child_weight=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=RANDOM_STATE,
    n_jobs=-1
)
xgb_model = evaluate_model('XGBoost', xgb_model,
                          X_train_scaled, X_test_scaled, y_train, y_test)

# Модель 4: Gradient Boosting
gb_model = GradientBoostingRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=5,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=RANDOM_STATE
)
gb_model = evaluate_model('Gradient Boosting', gb_model,
                         X_train_scaled, X_test_scaled, y_train, y_test)

# Создание таблицы сравнения
comparison_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Train R²': [results[m]['train_r2'] for m in results.keys()],
    'Test R²': [results[m]['test_r2'] for m in results.keys()],
    'Test RMSE': [results[m]['test_rmse'] for m in results.keys()],
    'Test MAE': [results[m]['test_mae'] for m in results.keys()],
    'CV R² (mean)': [results[m]['cv_mean'] for m in results.keys()],
    'CV R² (std)': [results[m]['cv_std'] for m in results.keys()]
})

comparison_df = comparison_df.round(4)
print("\n" + "="*80)
print("СРАВНЕНИЕ МОДЕЛЕЙ")
print("="*80)
print(comparison_df.to_string(index=False))
print("="*80)

# Визуализация сравнения моделей
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# R² Score
x_pos = np.arange(len(results))
train_r2 = [results[m]['train_r2'] for m in results.keys()]
test_r2 = [results[m]['test_r2'] for m in results.keys()]

axes[0, 0].bar(x_pos - 0.2, train_r2, 0.4, label='Train', alpha=0.8)
axes[0, 0].bar(x_pos + 0.2, test_r2, 0.4, label='Test', alpha=0.8)
axes[0, 0].set_xlabel('Модели')
axes[0, 0].set_ylabel('R² Score')
axes[0, 0].set_title('Сравнение R² Score', fontweight='bold')
axes[0, 0].set_xticks(x_pos)
axes[0, 0].set_xticklabels(results.keys(), rotation=45, ha='right')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# RMSE
test_rmse = [results[m]['test_rmse'] for m in results.keys()]
axes[0, 1].bar(x_pos, test_rmse, alpha=0.8, color='coral')
axes[0, 1].set_xlabel('Модели')
axes[0, 1].set_ylabel('RMSE')
axes[0, 1].set_title('Сравнение RMSE (Test)', fontweight='bold')
axes[0, 1].set_xticks(x_pos)
axes[0, 1].set_xticklabels(results.keys(), rotation=45, ha='right')
axes[0, 1].grid(True, alpha=0.3)

# MAE
test_mae = [results[m]['test_mae'] for m in results.keys()]
axes[1, 0].bar(x_pos, test_mae, alpha=0.8, color='lightgreen')
axes[1, 0].set_xlabel('Модели')
axes[1, 0].set_ylabel('MAE')
axes[1, 0].set_title('Сравнение MAE (Test)', fontweight='bold')
axes[1, 0].set_xticks(x_pos)
axes[1, 0].set_xticklabels(results.keys(), rotation=45, ha='right')
axes[1, 0].grid(True, alpha=0.3)

# Cross-Validation R²
cv_means = [results[m]['cv_mean'] for m in results.keys()]
cv_stds = [results[m]['cv_std'] for m in results.keys()]
axes[1, 1].bar(x_pos, cv_means, alpha=0.8, color='mediumpurple', yerr=cv_stds, capsize=5)
axes[1, 1].set_xlabel('Модели')
axes[1, 1].set_ylabel('CV R² Score')
axes[1, 1].set_title('Cross-Validation R² Score', fontweight='bold')
axes[1, 1].set_xticks(x_pos)
axes[1, 1].set_xticklabels(results.keys(), rotation=45, ha='right')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Scatter plots: Actual vs Predicted для каждой модели
fig, axes = plt.subplots(2, 2, figsize=(16, 14))
axes = axes.ravel()

for idx, (name, result) in enumerate(results.items()):
    y_pred = result['predictions']

    axes[idx].scatter(y_test, y_pred, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)

    # Идеальная линия
    min_val = min(y_test.min(), y_pred.min())
    max_val = max(y_test.max(), y_pred.max())
    axes[idx].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Идеальная линия')

    axes[idx].set_xlabel('Фактические значения', fontsize=11)
    axes[idx].set_ylabel('Предсказанные значения', fontsize=11)
    axes[idx].set_title(f'{name}\nR² = {result["test_r2"]:.4f}, RMSE = {result["test_rmse"]:.2f}',
                       fontsize=12, fontweight='bold')
    axes[idx].legend()
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

rf_importance = pd.DataFrame({
    'Feature': feature_columns,
    'Importance': models['Random Forest'].feature_importances_
}).sort_values('Importance', ascending=False)

print("\nВажность признаков (Random Forest):")
print(rf_importance)

# Визуализация
plt.figure(figsize=(12, 6))
plt.barh(range(len(rf_importance)), rf_importance['Importance'].values,
         color=plt.cm.viridis(np.linspace(0, 1, len(rf_importance))))
plt.yticks(range(len(rf_importance)), rf_importance['Feature'].values)
plt.xlabel('Важность признака', fontsize=12)
plt.title('Важность признаков - Random Forest', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()

xgb_importance = pd.DataFrame({
    'Feature': feature_columns,
    'Importance': models['XGBoost'].feature_importances_
}).sort_values('Importance', ascending=False)

print("\nВажность признаков (XGBoost):")
print(xgb_importance)

# Визуализация
plt.figure(figsize=(12, 6))
plt.barh(range(len(xgb_importance)), xgb_importance['Importance'].values,
         color=plt.cm.plasma(np.linspace(0, 1, len(xgb_importance))))
plt.yticks(range(len(xgb_importance)), xgb_importance['Feature'].values)
plt.xlabel('Важность признака', fontsize=12)
plt.title('Важность признаков - XGBoost', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()

# Сравнение важности признаков между моделями
importance_comparison = pd.DataFrame({
    'Feature': feature_columns,
    'Random Forest': models['Random Forest'].feature_importances_,
    'XGBoost': models['XGBoost'].feature_importances_,
    'Gradient Boosting': models['Gradient Boosting'].feature_importances_
})

# Нормализация для сравнения
for col in ['Random Forest', 'XGBoost', 'Gradient Boosting']:
    importance_comparison[col] = importance_comparison[col] / importance_comparison[col].sum()

importance_comparison = importance_comparison.sort_values('Random Forest', ascending=False)

# Визуализация
fig, ax = plt.subplots(figsize=(14, 8))
x = np.arange(len(importance_comparison))
width = 0.25

ax.bar(x - width, importance_comparison['Random Forest'], width, label='Random Forest', alpha=0.8)
ax.bar(x, importance_comparison['XGBoost'], width, label='XGBoost', alpha=0.8)
ax.bar(x + width, importance_comparison['Gradient Boosting'], width, label='Gradient Boosting', alpha=0.8)

ax.set_xlabel('Признаки', fontsize=12)
ax.set_ylabel('Нормализованная важность', fontsize=12)
ax.set_title('Сравнение важности признаков между моделями', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(importance_comparison['Feature'], rotation=45, ha='right')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

best_model_name = comparison_df.loc[comparison_df['Test R²'].idxmax(), 'Model']
best_model = models[best_model_name]

print(f"Лучшая модель: {best_model_name}")
print(f"Test R²: {results[best_model_name]['test_r2']:.4f}")

# Предсказание для всего датасета
X_all_scaled = scaler.transform(bio_extended[feature_columns])
bio_extended['predicted_biodiversity'] = best_model.predict(X_all_scaled)
bio_extended['prediction_error'] = bio_extended['biodiversity_index'] - bio_extended['predicted_biodiversity']

# Карта с предсказаниями
fig = px.choropleth(bio_extended,
                    locations='ISO3',
                    color='predicted_biodiversity',
                    hover_name='Country',
                    hover_data={
                        'biodiversity_index': ':.2f',
                        'predicted_biodiversity': ':.2f',
                        'prediction_error': ':.2f',
                        'ISO3': False
                    },
                    color_continuous_scale='Viridis',
                    labels={'predicted_biodiversity': 'Предсказанный индекс'})

fig.update_layout(
    title={
        'text': f'Предсказание биоразнообразия ({best_model_name})',
        'x': 0.5,
        'xanchor': 'center',
        'font': {'size': 20}
    },
    geo=dict(
        showframe=False,
        showcoastlines=True,
        projection_type='natural earth'
    ),
    height=600
)

fig.show()

# Карта ошибок предсказаний
fig = px.choropleth(bio_extended,
                    locations='ISO3',
                    color='prediction_error',
                    hover_name='Country',
                    hover_data={
                        'biodiversity_index': ':.2f',
                        'predicted_biodiversity': ':.2f',
                        'prediction_error': ':.2f',
                        'ISO3': False
                    },
                    color_continuous_scale='RdBu',
                    color_continuous_midpoint=0,
                    labels={'prediction_error': 'Ошибка предсказания'})

fig.update_layout(
    title={
        'text': 'Ошибки предсказания биоразнообразия',
        'x': 0.5,
        'xanchor': 'center',
        'font': {'size': 20}
    },
    geo=dict(
        showframe=False,
        showcoastlines=True,
        projection_type='natural earth'
    ),
    height=600
)

fig.show()

# Формирование аналитического отчета
print("\n" + "="*80)
print("Отчет: Факторы биоразнообразия регионов")
print("="*80)

print("\n1. Качество моделей:")
print("-" * 80)
for name in results.keys():
    print(f"\n{name}:")
    print(f"  - R² Score (Test): {results[name]['test_r2']:.4f}")
    print(f"  - RMSE (Test): {results[name]['test_rmse']:.4f}")
    print(f"  - Cross-Validation R²: {results[name]['cv_mean']:.4f} (±{results[name]['cv_std']:.4f})")

print(f"\n\nТоп модель: {best_model_name}")
print(f"  - Объясняет {results[best_model_name]['test_r2']*100:.2f}% вариации в биоразнообразии")

print("\n\n2. ключевые факторы:")
print("-" * 80)
top_features = xgb_importance.head(5)
for idx, row in top_features.iterrows():
    print(f"  {idx+1}. {row['Feature']}: {row['Importance']:.4f}")

print("\n\n3. Корреляционный анализ:")
print("-" * 80)
top_corr = corr_matrix['biodiversity_index'].sort_values(ascending=False)[1:6]
for feature, corr_val in top_corr.items():
    direction = "положительная" if corr_val > 0 else "отрицательная"
    print(f"  - {feature}: {corr_val:.4f} ({direction} связь)")

print("\n\n4. Рекомендации:")
print("-" * 80)
print("  1. Приоритет защиты территорий с высоким лесным покровом")
print("  2. Контроль урбанизации в регионах с высоким биоразнообразием")
print("  3. Снижение выбросов CO2 для сохранения экосистем")
print("  4. Создание охраняемых территорий в климатически благоприятных зонах")
print("  5. Мониторинг и контроль плотности населения в биоразнообразных регионах")

# Сохранение результатов
results_summary = pd.DataFrame({
    'Country': bio_extended['Country'],
    'Country_Code': bio_extended['Country_Code'],
    'Actual_Biodiversity': bio_extended['biodiversity_index'],
    'Predicted_Biodiversity': bio_extended['predicted_biodiversity'],
    'Error': bio_extended['prediction_error'],
    'Forest_Cover': bio_extended['forest_cover'],
    'Temperature': bio_extended['avg_temperature'],
    'Population_Density': bio_extended['population_density']
})

results_summary.to_csv('biodiversity_results.csv', index=False)
print("\nРезультаты сохранены в файл 'biodiversity_results.csv'")

